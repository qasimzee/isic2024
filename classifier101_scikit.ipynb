{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qasimzee/isic2024/blob/main/classifier101_scikit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ciSW7S7CFo6"
      },
      "outputs": [],
      "source": [
        "!pip install fastai h5py pillow\n",
        "!pip install scikit-learn==1.2.2\n",
        "!pip install numpy==1.26.4\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from fastai.vision.all import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rW5n1z14LOb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoBqBbTKOd5U"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/kaggle/isic-2024-data/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nwnl6Er6VtJY"
      },
      "outputs": [],
      "source": [
        "HDF5_FILE = \"/content/drive/MyDrive/kaggle/isic-2024-data/train-image.hdf5\"\n",
        "METADATA_FILE = \"/content/drive/MyDrive/kaggle/isic-2024-data/train-metadata.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JedudXJPOwqT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metadata CSV file\n",
        "metadata_df = pd.read_csv(METADATA_FILE)\n",
        "# Display the first few rows to understand the structure\n",
        "df_target_1 = metadata_df[metadata_df['target'] == 1]\n",
        "\n",
        "# Filter the DataFrame for entries where 'target' is 0 and select the first 5000\n",
        "df_target_0 = metadata_df[metadata_df['target'] == 0][:5000]\n",
        "\n",
        "# Concatenate the two DataFrames\n",
        "metadata_df = pd.concat([df_target_1, df_target_0])\n",
        "metadata_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df[metadata_df['target'] == 1]"
      ],
      "metadata": {
        "id": "5hi5TzhsNJVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG2xa4hghr4C"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming metadata_df has columns 'isic_id' and 'target'\n",
        "train_df, test_df = train_test_split(metadata_df, test_size=0.01, random_state=42)\n",
        "\n",
        "\n",
        "target_size = (56, 56)\n",
        "batch_size = 1024\n",
        "input_shape=(56, 56, 3)\n",
        "\n",
        "test_df[test_df['target'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWMktTwhlY4K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Setup ImageDataGenerator with augmentations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=60,            # Random rotations up to 30 degrees\n",
        "    width_shift_range=0.2,        # Horizontal shifts up to 10% of the width\n",
        "    height_shift_range=0.2,       # Vertical shifts up to 10% of the height\n",
        "    brightness_range=[0.8, 1.2],  # Random brightness adjustment\n",
        "    zoom_range=0.5,               # Random zooms up to 20%\n",
        "    horizontal_flip=True,         # Random horizontal flips\n",
        "    fill_mode='nearest'           # Fill mode for points outside the boundaries\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH8E5_29Yn-C"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_and_preprocess_images(image_bytes, target_size):\n",
        "\n",
        "  # Decode the bytes into an image array\n",
        "  image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "  # Resize the image to the target size\n",
        "  resized_image = cv2.resize(image, target_size)\n",
        "\n",
        "  # Normalize the image\n",
        "  normalized_image = resized_image / 255.0\n",
        "\n",
        "  # Convert list to NumPy array\n",
        "  features = np.array(normalized_image)\n",
        "\n",
        "  return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9usGG14ovvwY"
      },
      "outputs": [],
      "source": [
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def hdf5_data_generator(hdf5_file, metadata_df, batch_size, target_size=(56, 56)):\n",
        "  with h5py.File(hdf5_file, 'r') as hf:\n",
        "    num_samples = len(train_df)\n",
        "\n",
        "    while True:\n",
        "      for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "\n",
        "        batch_isic_ids = train_df['isic_id'].iloc[start:end]\n",
        "        batch_labels = train_df['target'].iloc[start:end].values\n",
        "\n",
        "        batch_images = []\n",
        "\n",
        "        for isic_id in batch_isic_ids:\n",
        "          image_bytes = hf[str(isic_id)][()]\n",
        "          features = load_and_preprocess_images(image_bytes, target_size)\n",
        "          batch_images.append(features)\n",
        "\n",
        "        # Apply augmentations using datagen\n",
        "        augmented_images = np.array([datagen.random_transform(image) for image in batch_images])\n",
        "\n",
        "        yield augmented_images, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv33TGW4j5Lv",
        "outputId": "8b126651-cc8e-4d9b-fa3f-317ec7cbc4fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/3\n",
            "\u001b[1m 43/166\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14:55\u001b[0m 7s/step - accuracy: 0.8632 - loss: 0.3375"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the pre-trained EfficientNetB0 model without the top layers\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')  # Single output for binary classification\n",
        "])\n",
        "\n",
        "# Calculate class weights based on the training set\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_df['target']), y=train_df['target'])\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(hdf5_data_generator(HDF5_FILE, train_df, batch_size),\n",
        "          steps_per_epoch=len(train_df) // 32,\n",
        "          epochs=3)  # Start with fewer epochs to see how it performs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y62WJySFwGUk"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "with h5py.File(HDF5_FILE, 'r') as hf:\n",
        "    num_samples = len(test_df)\n",
        "\n",
        "    for start in range(0, num_samples, batch_size):\n",
        "        end = min(start + batch_size, num_samples)\n",
        "        batch_df = test_df.iloc[start:end]\n",
        "        batch_isic_ids = batch_df['isic_id'].values\n",
        "        batch_images = []\n",
        "\n",
        "        for isic_id in batch_isic_ids:\n",
        "          image_bytes = hf[str(isic_id)][()]\n",
        "\n",
        "          # Decode the bytes into an image array\n",
        "          image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "          # Resize the image to the target size\n",
        "          resized_image = cv2.resize(image, target_size)\n",
        "\n",
        "          # Normalize the image\n",
        "          normalized_image = resized_image / 255.0\n",
        "\n",
        "          # Add a batch dimension to the image (from (56, 56, 3) to (1, 56, 56, 3))\n",
        "          image_batch = np.expand_dims(normalized_image, axis=0)\n",
        "\n",
        "          # Make a prediction\n",
        "          prediction = model.predict(image_batch) [0]\n",
        "          results.append({'isic_id': isic_id, 'target': prediction[0]})\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# results_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df[test_df['target'] == 1]"
      ],
      "metadata": {
        "id": "XFh1GEAwKqzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "IlGQGWnQO4Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnI-zIOIleBg"
      },
      "outputs": [],
      "source": [
        "# import joblib\n",
        "\n",
        "# # Save the trained model to a file\n",
        "# joblib_filename = 'random_forest_model.joblib'\n",
        "# joblib.dump(model, joblib_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9NUrlctlf5z"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# # Save the trained model to a file\n",
        "# pickle_filename = '/content/drive/MyDrive/kaggle/isic-2024-data/cnn.pkl'\n",
        "# with open(pickle_filename, 'wb') as file:\n",
        "#     pickle.dump(model, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhxKIaW7qSek"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# # Load the model from the pkl file\n",
        "# with open(pickle_filename, 'rb') as file:\n",
        "#     model = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReFTBcggrSuu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas.api.types\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80) -> float:\n",
        "    '''\n",
        "    2024 ISIC Challenge metric: pAUC\n",
        "\n",
        "    Given a solution file and submission file, this function returns the\n",
        "    the partial area under the receiver operating characteristic (pAUC)\n",
        "    above a given true positive rate (TPR) = 0.80.\n",
        "    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.\n",
        "\n",
        "    (c) 2024 Nicholas R Kurtansky, MSKCC\n",
        "\n",
        "    Args:\n",
        "        solution: ground truth pd.DataFrame of 1s and 0s\n",
        "        submission: solution dataframe of predictions of scores ranging [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        Float value range [0, max_fpr]\n",
        "    '''\n",
        "\n",
        "    del solution[row_id_column_name]\n",
        "    del submission[row_id_column_name]\n",
        "\n",
        "    # check submission is numeric\n",
        "    if not pandas.api.types.is_numeric_dtype(submission.values):\n",
        "        raise ParticipantVisibleError('Submission target column must be numeric')\n",
        "\n",
        "    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)\n",
        "    v_gt = abs(np.asarray(solution.values)-1)\n",
        "\n",
        "    # flip the submissions to their compliments\n",
        "    v_pred = -1.0*np.asarray(submission.values)\n",
        "    max_fpr = abs(1-min_tpr)\n",
        "\n",
        "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
        "    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)\n",
        "\n",
        "    if max_fpr is None or max_fpr == 1:\n",
        "        return auc(fpr, tpr)\n",
        "    if max_fpr <= 0 or max_fpr > 1:\n",
        "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
        "\n",
        "    # Add a single point at max_fpr by linear interpolation\n",
        "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
        "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
        "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
        "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
        "    fpr = np.append(fpr[:stop], max_fpr)\n",
        "\n",
        "    partial_auc = auc(fpr, tpr)\n",
        "\n",
        "    return(partial_auc)\n",
        "\n",
        "# solution = test_df[['isic_id', 'target']]\n",
        "# predictions = predictions_df[['isic_id', 'target']]\n",
        "\n",
        "# partial_auc = score(solution, predictions, \"isic_id\")\n",
        "# partial_auc"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbsBaVvToy8bIfiA1RGv+8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}